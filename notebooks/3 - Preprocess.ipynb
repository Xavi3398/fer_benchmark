{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c29405-b477-4ed2-ac6c-241b117a9c01",
   "metadata": {},
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c51644a-2113-493b-93cf-c0d106a3f55f",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "1. MiVOLO: detect keypoints\n",
    "2. cv2: to grayscale\n",
    "3. ndimage.shift: compute center of face and move it to the center of the image\n",
    "4. ndimage.rotate: compute angle between line of the eyes and horizontal plane and rotate\n",
    "5. crop: compute aspect ratio and use it to crop image\n",
    "6. ndimage.zoom: resize to always the same size\n",
    "7. normalize: histogram stretching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26811b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "dataset_root = 'C:/DATASETS/AGE-FER'\n",
    "dataset_imgs_path_in = os.path.join(dataset_root, 'images')\n",
    "dataset_imgs_path_debug = os.path.join(dataset_root, 'images-debug')\n",
    "csv_columns = ['dataset','user_id','name','class','age','gender','race','perspective', 'age_group', 'subset', 'auto_age', 'auto_gender', 'auto_perspective', 'age_group_clean', 'gaze']\n",
    "\n",
    "# Mode used for ndimage transformations\n",
    "MODE = 'reflect'\n",
    "\n",
    "# Image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "EYE_L = 0\n",
    "EYE_R = 1\n",
    "NOSE = 2\n",
    "MOUTH_L = 3\n",
    "MOUTH_R = 4\n",
    "\n",
    "def preprocess_img(img_name, path_in, path_out, path_no_face, debug=False, skip_face_detection=False, skip_landmarks=False, grayscale=False, img_size=224):\n",
    "    img = cv2.imread(os.path.join(path_in, img_name))\n",
    "\n",
    "    # To grayscale\n",
    "    if grayscale:\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "    else:\n",
    "        img_gray = img\n",
    "\n",
    "    if not skip_landmarks:\n",
    "\n",
    "        # Get keypoints\n",
    "        keypoints = get_keypoints(img, use_detector=not skip_face_detection)\n",
    "        if keypoints is None:\n",
    "            if path_no_face is not None:\n",
    "                shutil.copy(os.path.join(path_in, img_name), os.path.join(path_no_face, img_name))\n",
    "            return False\n",
    "\n",
    "        # Shift\n",
    "        middle_point = (img.shape[1]//2, img.shape[0]//2)\n",
    "        if grayscale:\n",
    "            img_gray = ndimage.shift(img_gray, (middle_point[1] - keypoints[NOSE][1], middle_point[0] - keypoints[NOSE][0]), mode=MODE)\n",
    "        else:\n",
    "            img_gray = ndimage.shift(img_gray, (middle_point[1] - keypoints[NOSE][1], middle_point[0] - keypoints[NOSE][0], 0), mode=MODE)\n",
    "        keypoints = translate_points(keypoints, (middle_point[0] - keypoints[NOSE][0], middle_point[1] - keypoints[NOSE][1]))\n",
    "\n",
    "        # Rotate\n",
    "        angle = get_eye_rotation(keypoints[EYE_L], keypoints[EYE_R])\n",
    "        img_gray = ndimage.rotate(img_gray, radians_to_degrees(angle), reshape=False, mode=MODE)\n",
    "        keypoints = rotate_points(keypoints, angle, middle_point)\n",
    "\n",
    "        # Crop\n",
    "        padding_h = np.linalg.norm(np.mean([keypoints[MOUTH_L], keypoints[MOUTH_R]], axis=0) - np.mean([keypoints[EYE_L], keypoints[EYE_R]], axis=0)) / 2\n",
    "        height = width = int(4 * padding_h)\n",
    "        eye_d = keypoints[EYE_R][0] - keypoints[EYE_L][0]\n",
    "        padding_w = (width - eye_d) / 2\n",
    "        p1_x = int(keypoints[EYE_L][0] - padding_w)\n",
    "        p1_y = int(keypoints[EYE_L][1] - padding_h)\n",
    "        p2_x = p1_x + width\n",
    "        p2_y = p1_y + height\n",
    "\n",
    "        # Add borders if bbox is out of bounds\n",
    "        left = top = right = bottom = 0\n",
    "\n",
    "        if p1_x < 0:\n",
    "            left = -p1_x\n",
    "\n",
    "        if p1_y < 0:\n",
    "            top = -p1_y\n",
    "\n",
    "        if p2_x > img_gray.shape[1]:\n",
    "            right = p2_x - img_gray.shape[1]\n",
    "\n",
    "        if p2_y > img_gray.shape[0]:\n",
    "            bottom = p2_y - img_gray.shape[0]\n",
    "\n",
    "        if left > 0 or top > 0 or right > 0 or bottom > 0:\n",
    "            img_gray = cv2.copyMakeBorder(img_gray, top, bottom, left, right, cv2.BORDER_REFLECT_101)\n",
    "\n",
    "        # Update keypoints\n",
    "        bbox = (p1_x+left, p1_y+top, p2_x+left, p2_y+top)\n",
    "        keypoints = translate_points(keypoints, (-p1_x, -p1_y))\n",
    "\n",
    "        # Crop\n",
    "        img_gray = crop_img(img_gray, bbox)\n",
    "\n",
    "    # Zero size check\n",
    "    if img_gray.size == 0:\n",
    "        if path_no_face is not None:\n",
    "            shutil.copy(os.path.join(path_in, img_name), os.path.join(path_no_face, img_name))\n",
    "        print('Zero size:', img_name)\n",
    "        return False\n",
    "\n",
    "    # Resize\n",
    "    zoom = np.array([img_size, img_size]) / img_gray.shape[:2]\n",
    "    if grayscale:\n",
    "        img_gray = ndimage.zoom(img_gray, zoom, mode=MODE)\n",
    "    else:\n",
    "        img_gray = ndimage.zoom(img_gray, (zoom[0], zoom[1], 1), mode=MODE)\n",
    "    \n",
    "    # Resize keypoints\n",
    "    if not skip_landmarks:\n",
    "        keypoints = resize_points(keypoints, zoom)\n",
    "\n",
    "    # Zero size check\n",
    "    if img_gray.size == 0 or img_gray.shape[0] != img_size or img_gray.shape[1] != img_size:\n",
    "        if path_no_face is not None:\n",
    "            shutil.copy(os.path.join(path_in, img_name), os.path.join(path_no_face, img_name))\n",
    "        print('Zero size (2):', img_name)\n",
    "        return False\n",
    "    \n",
    "    # Normalize\n",
    "    img_gray = img_to_float(img_gray)\n",
    "    img_gray = histogram_stretching(img_gray)\n",
    "    img_gray = img_to_uint8(img_gray)\n",
    "\n",
    "    if debug:\n",
    "        img_gray = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)\n",
    "        if not skip_landmarks:\n",
    "            for kp in keypoints:\n",
    "                cv2.circle(img_gray, (int(kp[0]), int(kp[1])), radius=int(img_size/30), color=(0,0,255), thickness=-1)\n",
    "        \n",
    "    # Save result\n",
    "    cv2.imwrite(os.path.join(path_out, img_name), img_gray)\n",
    "    return True\n",
    "\n",
    "def preprocess(dataset_imgs_path_out, dataset_imgs_path_no_face, dataset_labels_path, dataset_labels_path_out, dataset_labels_path_no_face, grayscale=True, img_size=224):\n",
    "    \n",
    "    # Create output folders\n",
    "    if not os.path.exists(dataset_imgs_path_out):\n",
    "        os.mkdir(dataset_imgs_path_out)\n",
    "    if not os.path.exists(dataset_imgs_path_no_face):\n",
    "        os.mkdir(dataset_imgs_path_no_face)\n",
    "\n",
    "    with open(dataset_labels_path, 'r') as csv_input, open(dataset_labels_path_out, 'w', newline='') as csv_output, open(dataset_labels_path_no_face, 'w', newline='') as csv_no_face:\n",
    "\n",
    "        # CSV reader and writer\n",
    "        reader = csv.DictReader(csv_input, delimiter=',', quotechar='\"')\n",
    "        writer = csv.DictWriter(csv_output, delimiter=',', quotechar='\"', fieldnames=csv_columns)\n",
    "        writer_no_face = csv.DictWriter(csv_no_face, delimiter=',', quotechar='\"', fieldnames=csv_columns)\n",
    "\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "        writer_no_face.writeheader()\n",
    "\n",
    "        # Count lines and reset reader\n",
    "        total_lines = sum(1 for _ in reader)\n",
    "        csv_input.seek(0)\n",
    "        reader = csv.DictReader(csv_input, delimiter=',', quotechar='\"')\n",
    "\n",
    "        # Process each row\n",
    "        for row in tqdm(reader, total=total_lines):\n",
    "\n",
    "            # Preprocess image if not already done\n",
    "            if not os.path.exists(os.path.join(dataset_imgs_path_out, row['name'])) and not os.path.exists(os.path.join(dataset_imgs_path_no_face, row['name'])):\n",
    "                \n",
    "                if row['dataset'] in ['AffectNet', 'RAF-DB', 'NHFI', 'FER2013', 'ExpW', 'Google-FE-Test']:\n",
    "                    skip_face_detection = True\n",
    "                else:\n",
    "                    skip_face_detection = False\n",
    "\n",
    "                if preprocess_img(row['name'], dataset_imgs_path_in, dataset_imgs_path_out, dataset_imgs_path_no_face, debug=False, skip_face_detection=skip_face_detection, skip_landmarks=False, grayscale=grayscale, img_size=img_size):\n",
    "                    writer.writerow(row)\n",
    "                else:\n",
    "                    writer_no_face.writerow(row)\n",
    "            else:\n",
    "                if os.path.exists(os.path.join(dataset_imgs_path_out, row['name'])):\n",
    "                    writer.writerow(row)\n",
    "                else:\n",
    "                    writer_no_face.writerow(row)\n",
    "\n",
    "def get_eye_rotation(eye1, eye2):\n",
    "    v_eyes = get_vector(eye1, eye2)\n",
    "    angle = get_angle(v_eyes, (1, 0))\n",
    "    return angle if v_eyes[1] > 0 else -angle\n",
    "\n",
    "def get_angle(v1, v2):\n",
    "    \"\"\"Get angle between two vectors.\"\"\"\n",
    "    return math.acos(np.dot(v1, v2) / (magnitude(v1) * magnitude(v2)))\n",
    "\n",
    "def get_unit_vector(v):\n",
    "    \"\"\"Normalize vector\"\"\"\n",
    "    m = magnitude(v)\n",
    "    return (v[0]/m, v[1]/m)\n",
    "    \n",
    "def get_vector(p1, p2):\n",
    "    \"\"\"Get vector between two points.\"\"\"\n",
    "    return get_unit_vector((p2[0] - p1[0], p2[1] - p1[1]))\n",
    "\n",
    "def magnitude(v): \n",
    "    \"\"\"Get magnitude of a vector.\"\"\"\n",
    "    return math.sqrt(sum(pow(x, 2) for x in v))\n",
    "\n",
    "def crop_img(img, bbox):\n",
    "    \"\"\"Crop an image by a bbox.\"\"\"\n",
    "    return img[bbox[1]:bbox[3], bbox[0]:bbox[2], ...]\n",
    "\n",
    "def resize_points(keypoints, zoom):\n",
    "    return [(kp[0] * zoom[0], kp[1] * zoom[1]) for kp in keypoints]\n",
    "\n",
    "def translate_points(keypoints, movement):\n",
    "    return [(kp[0] + movement[0], kp[1] + movement[1]) for kp in keypoints]\n",
    "    \n",
    "def rotate_points(keypoints, radians, center):\n",
    "    \"\"\"Rotate all keypoints around the origin (0, 0).\"\"\"\n",
    "    \n",
    "    # Translate to origin\n",
    "    keypoints = translate_points(keypoints, (-center[0], -center[1]))\n",
    "    \n",
    "    # Rotate each point around origin\n",
    "    keypoints = [rotate_point_origin(kp, radians) for kp in keypoints]\n",
    "    \n",
    "    # Translate back\n",
    "    keypoints = translate_points(keypoints, (center[0], center[1]))\n",
    "    return keypoints\n",
    "\n",
    "def rotate_point_origin(xy, radians):\n",
    "    \"\"\"Rotate a point around the origin (0, 0).\"\"\"\n",
    "    x, y = xy\n",
    "    xx = x * math.cos(radians) + y * math.sin(radians)\n",
    "    yy = -x * math.sin(radians) + y * math.cos(radians)\n",
    "\n",
    "    return xx, yy\n",
    "\n",
    "def radians_to_degrees(rad):\n",
    "    return rad * 180 / math.pi\n",
    "\n",
    "def show_img(img):\n",
    "    if len(img.shape) > 1:\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_GRAY2RGB))\n",
    "\n",
    "def histogram_stretching(img, h_min=0, h_max=1):\n",
    "    max_value = np.max(img)\n",
    "    min_value = np.min(img)\n",
    "    if max_value > 0 and min_value != max_value:\n",
    "        return h_min+(h_max-h_min)*(img-min_value)/(max_value-min_value)\n",
    "    else:\n",
    "        return img\n",
    "    \n",
    "def img_to_uint8(img):\n",
    "    return (img * 255).astype('uint8')\n",
    "\n",
    "def img_to_float(img):\n",
    "    return img / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d0fdc",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c3357-3031-4c38-b3ba-b10d6544ea4a",
   "metadata": {},
   "source": [
    "### Using SPIGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c103733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 268 layers, 68125494 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPIGA model loaded!\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.yolo.engine.model import YOLO\n",
    "from spiga.inference.config import ModelConfig\n",
    "from spiga.inference.framework import SPIGAFramework\n",
    "\n",
    "weights = '../weights/yolov8x_person_face.pt'\n",
    "yolo = YOLO(weights)\n",
    "yolo.fuse()\n",
    "\n",
    "dataset = 'wflw'\n",
    "cfg = ModelConfig(dataset)\n",
    "cfg.load_model_url = None\n",
    "cfg.model_weights_path = '../weights'\n",
    "cfg.model_weights = 'spiga_wflw.pt'\n",
    "processor = SPIGAFramework(cfg)\n",
    "\n",
    "def get_bbox_hw(img):\n",
    "    \n",
    "    # YOLO detect face\n",
    "    yolo_pred = yolo(img, conf=.4, iou=.7, half=True, verbose=False)\n",
    "    \n",
    "    if len(yolo_pred) < 1:\n",
    "        return None\n",
    "    \n",
    "    yolo_pred = yolo_pred[0].boxes\n",
    "    classes = yolo_pred.cls.numpy(force=True)\n",
    "    bboxes = yolo_pred.xyxy.numpy(force=True)\n",
    "    \n",
    "    if not 1 in classes:\n",
    "        return None\n",
    "    \n",
    "    face_bbox = bboxes[np.where(classes == 1)[0]][0].astype('int')\n",
    "    face_bbox_hw = np.array([face_bbox[0], face_bbox[1], face_bbox[2] - face_bbox[0], face_bbox[3] - face_bbox[1]])\n",
    "    return face_bbox_hw\n",
    "\n",
    "def get_spiga_feature(img, use_detector=True, feature='landmarks'):\n",
    "    \n",
    "    if use_detector:\n",
    "        face_bbox_hw = get_bbox_hw(img)\n",
    "    else:\n",
    "        face_bbox_hw = [0, 0, img.shape[1], img.shape[0]]\n",
    "    \n",
    "    if face_bbox_hw is None:\n",
    "        print('No face detected.')\n",
    "        return None\n",
    "\n",
    "    features = processor.inference(img, [face_bbox_hw])\n",
    "    if features is None:\n",
    "        return None\n",
    "    if features[feature] is None:\n",
    "        return None\n",
    "    return np.array(features[feature][0])\n",
    "\n",
    "def get_keypoints(img, use_detector=True):\n",
    "    landmarks = get_spiga_feature(img, use_detector, 'landmarks')\n",
    "    if landmarks is None:\n",
    "        return None\n",
    "    return [np.mean(landmarks[60:68], axis=0), np.mean(landmarks[68:76], axis=0), landmarks[53], landmarks[88], landmarks[92]]\n",
    "\n",
    "def get_pose(img, use_detector=True):\n",
    "    return get_spiga_feature(img, use_detector, 'headpose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a25f5-d88c-4a35-86d5-60d1652d2fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 444517/446365 [00:45<00:00, 17069.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\pytorch_fer\\Lib\\site-packages\\torch\\nn\\functional.py:4434: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\Anaconda3\\envs\\pytorch_fer\\Lib\\site-packages\\torch\\nn\\functional.py:4373: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "100%|██████████| 446365/446365 [04:23<00:00, 1695.50it/s] \n"
     ]
    }
   ],
   "source": [
    "dataset_labels_path = os.path.join(dataset_root, 'labels5 - excluded no age.csv')\n",
    "dataset_labels_path_out = os.path.join(dataset_root, '24-datasets.csv')\n",
    "dataset_labels_path_no_face = os.path.join(dataset_root, 'labels6 - no-face.csv')\n",
    "dataset_imgs_path_out = os.path.join(dataset_root, 'images-preprocessed')\n",
    "dataset_imgs_path_no_face = os.path.join(dataset_root, 'images-no-face')\n",
    "\n",
    "preprocess(dataset_imgs_path_out, dataset_imgs_path_no_face, dataset_labels_path, dataset_labels_path_out, dataset_labels_path_no_face, grayscale=True, img_size=IMG_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_fer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
