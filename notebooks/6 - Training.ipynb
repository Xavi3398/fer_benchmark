{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = 'C:/DATASETS/AGE-FER'\n",
    "dataset_imgs_path = os.path.join(dataset_root, 'images-preprocessed')\n",
    "full_dataset_labels_path = os.path.join(dataset_root, '24-datasets.csv')\n",
    "\n",
    "labels = ['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise', 'neutral']\n",
    "label_map = { 'anger': 0,\n",
    "              'disgust': 1,\n",
    "              'fear': 2,\n",
    "              'happiness': 3,\n",
    "              'sadness': 4,\n",
    "              'surprise': 5,\n",
    "              'neutral': 6}\n",
    "\n",
    "dtypes = {\n",
    "    'dataset': 'category',\n",
    "    'user_id': 'category',\n",
    "    'name': str,\n",
    "    'class': 'category',\n",
    "    'age': 'Int8',\n",
    "    'gender':'category' ,\n",
    "    'race': 'category',\n",
    "    'perspective': 'category',\n",
    "    'age_group': 'category',\n",
    "    'subset': 'category',\n",
    "    'auto_age': bool,\n",
    "    'auto_gender': bool,\n",
    "    'age_group_clean': 'category',\n",
    "    'gaze': 'category',\n",
    "    'auto_perspective': bool,\n",
    "    'key': 'category'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    def __init__(self, annotations_file, dtypes, img_dir, transform=None, target_transform=None, subdataset=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file, dtype=dtypes, sep=',', quotechar='\"')\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        if subdataset:\n",
    "            self.img_labels = self.img_labels[self.img_labels['dataset'] == subdataset]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, self.img_labels.columns.get_loc(\"name\")]).lower()\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.img_labels.iloc[idx, self.img_labels.columns.get_loc(\"class\")]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label, self.img_labels.iloc[idx, self.img_labels.columns.get_loc(\"age_group_clean\")]\n",
    "\n",
    "def display_image_and_label(dataloader, labels):\n",
    "    train_features, train_labels = next(iter(dataloader))\n",
    "    print(f\"Feature batch shape: {train_features.size()}\")\n",
    "    print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "    img = train_features[0].squeeze()\n",
    "    label = train_labels[0]\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(f\"Label: {labels[label]}\")\n",
    "\n",
    "def get_model(model_name, num_classes, pretraining_dataset='IMAGENET1K_V1', in_channels=3):\n",
    "    if model_name == 'ConvNeXt_Small':\n",
    "        model = models.convnext_small(weights=pretraining_dataset)\n",
    "        model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n",
    "        model.features[0][0].in_channels = in_channels\n",
    "    elif model_name == 'Swin_S':\n",
    "        model = models.swin_s(weights=pretraining_dataset)\n",
    "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        model.features[0][0].in_channels = in_channels\n",
    "    \n",
    "    transform = models.get_weight(model_name + '_Weights.'+pretraining_dataset).transforms()\n",
    "    \n",
    "    return model, transform\n",
    "\n",
    "def get_transform(default_transform, mode='train', augment=True):\n",
    "\n",
    "    aug_transform = [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "            transforms.ColorJitter(brightness=0.25, contrast=0.25)] if augment else []\n",
    "\n",
    "    if mode == 'train':\n",
    "        return transforms.Compose(aug_transform + [default_transform])\n",
    "    elif mode == 'test':\n",
    "        return default_transform\n",
    "    elif mode == 'augment':\n",
    "        return transforms.Compose(aug_transform)\n",
    "    else:\n",
    "        raise ValueError('Invalid mode')\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, verbose=True, f_logs=None):\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_classes = dataloader.dataset.img_labels['class'].unique().tolist()\n",
    "    print(f\"Num classes: {len(num_classes)} - {num_classes}\", file=f_logs)\n",
    "\n",
    "    accuracy = MulticlassAccuracy(num_classes=7, average='micro')\n",
    "    f1_scores = MulticlassF1Score(num_classes=7, average=None)\n",
    "\n",
    "    for batch, (X, y, age_group) in enumerate(tqdm(dataloader)):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y, age_group)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy.update(pred, y)\n",
    "        f1_scores.update(pred, y)\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    accuracy = accuracy.compute()\n",
    "    f1_scores = f1_scores.compute()\n",
    "    macro_f1_score = np.mean([f1_scores[label_map[label]] for label in num_classes])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Avg loss: {train_loss:>8f}. Accuracy: {accuracy:>8f}. Macro F1 Score: {macro_f1_score:>8f}\", file=f_logs)\n",
    "        print('F1 scores.', [f'{label}: {f1_scores[label_map[label]]:>8f}' for label in labels], file=f_logs)\n",
    "    return train_loss, accuracy, macro_f1_score\n",
    "\n",
    "def val(dataloader, model, loss_fn, verbose=True, f_logs=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    num_classes = dataloader.dataset.img_labels['class'].unique().tolist()\n",
    "    print(f\"Num classes: {len(num_classes)} - {num_classes}\", file=f_logs)\n",
    "    accuracy = MulticlassAccuracy(num_classes=7, average='micro')\n",
    "    f1_scores = MulticlassF1Score(num_classes=7, average=None)\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y, age_group) in enumerate(tqdm(dataloader)):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y, age_group).item()\n",
    "            accuracy.update(pred, y)\n",
    "            f1_scores.update(pred, y)\n",
    "    val_loss /= num_batches\n",
    "    accuracy = accuracy.compute()\n",
    "    f1_scores = f1_scores.compute()\n",
    "    macro_f1_score = np.mean([f1_scores[label_map[label]] for label in num_classes])\n",
    "    if verbose:\n",
    "        print(f\"Avg loss: {val_loss:>8f}. Accuracy: {accuracy:>8f}. Macro F1 Score: {macro_f1_score:>8f}\", file=f_logs)\n",
    "        print('F1 scores.', [f'{label}: {f1_scores[label_map[label]]:>8f}' for label in labels], file=f_logs)\n",
    "    return val_loss, accuracy, macro_f1_score\n",
    "\n",
    "def epochs_loop(train_dataloader, test_dataloader, model, loss_fn, optimizer, model_name, \n",
    "                subdataset, k, epochs_path, model_path, max_epochs, \n",
    "                use_patience = True, patience = 5, patience_th = 0.01, metric = 'val_acc', f_logs=None):\n",
    "\n",
    "    # Create pandas dataframe to store results\n",
    "    epochs_df = pd.DataFrame(columns=['model', 'subdataset', 'k', 'aug', 'epoch', 'train_loss', 'train_acc', 'train_f1', 'val_loss', 'val_acc', 'val_f1'])\n",
    "\n",
    "    # Initialize early stopping variables\n",
    "    counter_patience = 0\n",
    "    current_value = 1000 if 'loss' in metric else 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        print(\"-------------------------------\", file=f_logs)        \n",
    "        print(f\"Epoch [{epoch+1:>{len(str(max_epochs))}}/{max_epochs}]\", file=f_logs)\n",
    "        \n",
    "        print(\"Train:\", file=f_logs)\n",
    "        train_loss, train_acc, train_f1 = train(train_dataloader, model, loss_fn, optimizer, verbose=True, f_logs=f_logs)\n",
    "\n",
    "        print(\"Validate:\", file=f_logs)\n",
    "        val_loss, val_acc, val_f1 = val(test_dataloader, model, loss_fn, verbose=True, f_logs=f_logs)\n",
    "\n",
    "        if f_logs is not None:\n",
    "            f_logs.flush()\n",
    "        \n",
    "        # Store results to dataframe using concat\n",
    "        epochs_df.loc[len(epochs_df)] = {\n",
    "            'model': model_name, \n",
    "            'subdataset': subdataset, \n",
    "            'k': k, \n",
    "            'epoch': epoch+1, \n",
    "            'train_loss': train_loss, \n",
    "            'train_acc': float(train_acc), \n",
    "            'train_f1': float(train_f1), \n",
    "            'val_loss': val_loss, \n",
    "            'val_acc': float(val_acc),\n",
    "            'val_f1': float(val_f1)}\n",
    "        \n",
    "        if use_patience:\n",
    "\n",
    "            # Select current value\n",
    "            if metric == 'val_acc':\n",
    "                value = val_acc\n",
    "            elif metric == 'val_f1':\n",
    "                value = val_f1\n",
    "            elif metric == 'val_loss':\n",
    "                value = val_loss\n",
    "            elif metric == 'train_acc':\n",
    "                value = train_acc\n",
    "            elif metric == 'train_loss':\n",
    "                value = train_loss\n",
    "            elif metric == 'train_f1':\n",
    "                value = train_f1\n",
    "            \n",
    "            if 'loss' in metric:\n",
    "                # Better value: save model\n",
    "                if current_value - value > patience_th:\n",
    "                    counter_patience = 0\n",
    "                    current_value = value\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                # Worse value: increase patience counter\n",
    "                else:\n",
    "                    counter_patience += 1\n",
    "            else:\n",
    "                # Better value: save model\n",
    "                if value - current_value > patience_th:\n",
    "                    counter_patience = 0\n",
    "                    current_value = value\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                # Worse value: increase patience counter\n",
    "                else:\n",
    "                    counter_patience += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if counter_patience >= patience:\n",
    "                epoch = epoch - patience\n",
    "                print(f\"Early stopping at epoch {epoch+1}.\", file=f_logs)\n",
    "                break\n",
    "    \n",
    "    # Save model if not using patience\n",
    "    if not use_patience:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save dataframe to csv\n",
    "    epochs_df.to_csv(epochs_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_file = 'training-logs.txt'\n",
    "counter = 1\n",
    "while os.path.exists(logs_file):\n",
    "    logs_file = f'training-logs_{counter}.txt'\n",
    "    counter += 1\n",
    "\n",
    "with open(logs_file, 'w') as f_logs:\n",
    "\n",
    "    training_dir = os.path.join(dataset_root, 'training')\n",
    "    chosen_models = ['ConvNeXt_Small', 'Swin_S']\n",
    "    chosen_augmentation = [True]\n",
    "    chosen_balancing = ['weighted-loss']\n",
    "    epochs = 20\n",
    "    metric = 'val_f1'\n",
    "    patience_th = 0.01\n",
    "    patience = 5\n",
    "\n",
    "    # Get distinct datasets\n",
    "    df = pd.read_csv(full_dataset_labels_path, dtype=dtypes, sep=',', quotechar='\"')\n",
    "    subdatasets = list(df['dataset'].unique())\n",
    "\n",
    "    # Set device\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using {device} device\", file=f_logs)\n",
    "\n",
    "    # Create folders\n",
    "    if not os.path.exists(training_dir):\n",
    "        os.makedirs(training_dir)\n",
    "    models_dir = os.path.join(training_dir, 'models')\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    epochs_dir = os.path.join(training_dir, 'epochs')\n",
    "    if not os.path.exists(epochs_dir):\n",
    "        os.makedirs(epochs_dir)\n",
    "\n",
    "    for subdataset in subdatasets:\n",
    "        for k in range(1, 6):\n",
    "            for model_name in chosen_models:\n",
    "                print(f\"Model: {model_name}, Subdataset: {subdataset}, CV: {k}.\", file=f_logs)\n",
    "                \n",
    "                base_name = f'{model_name}_{subdataset}_{k}'\n",
    "                model_path = os.path.join(models_dir, f'{base_name}.pth')\n",
    "                epochs_path = os.path.join(epochs_dir, f'{base_name}.csv')\n",
    "                \n",
    "                if not os.path.exists(epochs_path):\n",
    "\n",
    "                    # Define model, load pre-trained weights and get default transform\n",
    "                    model, default_transform = get_model(model_name, len(labels), 'IMAGENET1K_V1')\n",
    "                    model = model.to(device)\n",
    "\n",
    "                    # Define optimizer\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "                    # Load the training dataset\n",
    "                    training_data = FERDataset(\n",
    "                        os.path.join(dataset_root, 'cv-labels', f'19-datasets_train_{k}.csv'),\n",
    "                        dtypes,\n",
    "                        dataset_imgs_path,\n",
    "                        transform=get_transform(default_transform, mode='train', augment=True),\n",
    "                        target_transform=lambda label: label_map[label],\n",
    "                        subdataset=subdataset)\n",
    "\n",
    "                    # Load the test dataset\n",
    "                    test_data = FERDataset(\n",
    "                        os.path.join(dataset_root, 'cv-labels', f'19-datasets_test_{k}.csv'),\n",
    "                        dtypes,\n",
    "                        dataset_imgs_path,\n",
    "                        transform=default_transform,\n",
    "                        target_transform=lambda label: label_map[label], \n",
    "                        subdataset=subdataset)\n",
    "\n",
    "                    # Count class counts for the dataset\n",
    "                    counts = training_data.img_labels.groupby(['class', 'age_group_clean']).size().unstack(fill_value=0).to_dict()\n",
    "\n",
    "                    # Get minimum class count, greater than 0\n",
    "                    class_counts = training_data.img_labels['class'].value_counts()\n",
    "                    class_counts = class_counts.reindex(labels)\n",
    "\n",
    "                    # Get minimum class count, greater than 0\n",
    "                    min_class_count = class_counts[class_counts > 0].min()\n",
    "\n",
    "                    # Define loss function\n",
    "                    weights = torch.tensor([min_class_count/class_counts[i] if class_counts[i] > 0 else .0 for i in range(len(labels))], dtype=torch.float32).to(device)\n",
    "                    print(f\"Weights: {weights.tolist()}\", file=f_logs)\n",
    "                    loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "                    sampler = None\n",
    "\n",
    "                    # Create dataloaders\n",
    "                    train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "                    test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "                    # Run training loop\n",
    "                    f_logs.flush()\n",
    "                    epochs_loop(train_dataloader, test_dataloader, model, loss_fn, optimizer, model_name, \n",
    "                                subdataset, k, True, epochs_path, model_path, \n",
    "                                epochs, use_patience=True, patience=patience, \n",
    "                                patience_th=patience_th, metric=metric, f_logs=f_logs)\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check training loss evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = os.path.join(dataset_root, 'training')\n",
    "epochs_dir = os.path.join(training_dir, 'epochs')\n",
    "all_epochs_file = os.path.join(epochs_dir, 'all_trainings.csv')\n",
    "chosen_models = ['ConvNeXt_Small', 'Swin_S']\n",
    "\n",
    "# Get distinct datasets\n",
    "df = pd.read_csv(full_dataset_labels_path, dtype=dtypes, sep=',', quotechar='\"')\n",
    "subdatasets = list(df['dataset'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all epoch logs into one CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe to store results\n",
    "all_epochs_df = pd.DataFrame(columns=['model', 'subdataset', 'k', 'epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "for subdataset in subdatasets:\n",
    "    for k in range(1, 6):\n",
    "        for model_name in chosen_models:\n",
    "\n",
    "            # CSV file name\n",
    "            base_name = f'{model_name}_{subdataset}_{k}'\n",
    "            epochs_path = os.path.join(epochs_dir, f'{base_name}.csv')\n",
    "\n",
    "            # Load dataframe\n",
    "            df = pd.read_csv(epochs_path, sep=',', quotechar='\"')\n",
    "\n",
    "            # Append to dataframe\n",
    "            all_epochs_df = pd.concat([all_epochs_df, df], ignore_index=True)\n",
    "\n",
    "# Save results\n",
    "all_epochs_df.to_csv(all_epochs_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_fer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
